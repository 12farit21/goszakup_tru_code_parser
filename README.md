# Goszakup Tender Links Scraper

Playwright-based web scraper для автоматического извлечения ссылок на тендеры с портала государственных закупок Казахстана (goszakup.gov.kz).

## Возможности

- Автоматическое изменение количества записей на странице (до 2000)
- Многостраничный парсинг с автоматической пагинацией
- Автоматическое удаление дубликатов ссылок
- Инкрементальное сохранение прогресса (контрольные точки)
- Настраиваемый лимит на количество ссылок
- Обработка ошибок с повторными попытками
- Корректное завершение при прерывании (Ctrl+C)
- Логирование в консоль и файл

## Установка

### 1. Клонирование репозитория (или переход в директорию проекта)

```bash
cd c:\CODES\Tender2_additional\goszakup_tru_code_parsing
```

### 2. Установка зависимостей

```bash
pip install -r requirements.txt
```

### 3. Установка браузера для Playwright

```bash
playwright install chromium
```

### 4. Настройка конфигурации

Скопируйте файл `.env.example` в `.env`:

```bash
copy .env.example .env
```

При необходимости отредактируйте `.env` файл для изменения параметров:

```env
# Основные параметры
BASE_URL=https://goszakup.gov.kz/ru/search/lots
RECORDS_PER_PAGE=2000
DEFAULT_MAX_LINKS=10000

# Задержка между страницами (секунды)
PAGE_DELAY=2.5

# Настройки браузера
HEADLESS_MODE=true
```

## Использование

### Базовый запуск

Запуск с параметрами по умолчанию из `.env`:

```bash
python scraper.py
```

### Ограничение количества ссылок

Парсинг максимум 2500 ссылок:

```bash
python scraper.py --max-links 2500
```

### Начало с определенной страницы

Начать со страницы 5 и собрать максимум 1000 ссылок:

```bash
python scraper.py --start-page 5 --max-links 1000
```

### Видимый режим браузера (для отладки)

Запуск с видимым окном браузера:

```bash
python scraper.py --max-links 100 --no-headless --verbose
```

### Подробное логирование

Включить режим DEBUG для детального логирования:

```bash
python scraper.py --max-links 100 --verbose
```

## Параметры командной строки

| Параметр | Описание | По умолчанию |
|----------|----------|--------------|
| `--max-links N` | Максимальное количество ссылок для парсинга | Из .env (10000) |
| `--start-page N` | Номер стартовой страницы | Из .env (1) |
| `--headless` | Запуск браузера в headless режиме | Из .env (true) |
| `--no-headless` | Запуск браузера с видимым окном | - |
| `--verbose` | Включить подробное логирование (DEBUG) | INFO |

## Формат выходных данных

Скрипт создает JSON файл в директории `output/` с именем формата `goszakup_links_YYYYMMDD_HHMMSS.json`:

```json
{
  "metadata": {
    "scrape_date": "2026-01-24T10:30:00",
    "total_links": 2547,
    "pages_scraped": 2,
    "records_per_page": 2000,
    "base_url": "https://goszakup.gov.kz/ru/search/lots",
    "filters": "filter[amount_from]=5000000&filter[trade_type]=g"
  },
  "links": [
    "https://goszakup.gov.kz/ru/announce/index/16099116",
    "https://goszakup.gov.kz/ru/announce/index/16099079",
    ...
  ]
}
```

## Логирование

Логи сохраняются в двух местах:

1. **Консоль** - вывод основных сообщений о прогрессе
2. **Файл** - в директории `logs/` с именем `scraper_YYYYMMDD_HHMMSS.log`

Формат логов:

```
2026-01-24 10:30:45 - INFO - Scraper initialized with max_links=5000
2026-01-24 10:30:50 - INFO - Page 1: Set records per page to 2000
2026-01-24 10:31:05 - INFO - Page 1: Extracted 2000 links (total: 2000)
2026-01-24 10:31:10 - INFO - Checkpoint saved: output/goszakup_links_20260124_103045.json
```

## Контрольные точки

Скрипт автоматически сохраняет прогресс каждые N страниц (по умолчанию: 10). Это позволяет:

- Избежать потери данных при сбоях
- Прервать работу (Ctrl+C) и сохранить собранные ссылки
- Продолжить парсинг позже

Параметр `CHECKPOINT_INTERVAL` в `.env` контролирует частоту сохранений.

## Обработка ошибок

Скрипт автоматически обрабатывает следующие ситуации:

- **Сетевые ошибки** - повторные попытки с экспоненциальной задержкой
- **Отсутствующие элементы** - сохранение скриншота для отладки, переход к следующей странице
- **Прерывание пользователем (Ctrl+C)** - корректное сохранение прогресса
- **Конец результатов** - автоматическое определение последней страницы

## Настройка конфигурации

Все важные параметры хранятся в `.env` файле:

### URL конфигурация

```env
BASE_URL=https://goszakup.gov.kz/ru/search/lots
URL_FILTERS=filter%5Bamount_from%5D=5000000&filter%5Btrade_type%5D=g
```

### Параметры парсинга

```env
RECORDS_PER_PAGE=2000          # Количество записей на странице (макс: 2000)
START_PAGE=1                   # Стартовая страница
DEFAULT_MAX_LINKS=10000        # Лимит по умолчанию
```

### Производительность

```env
CHECKPOINT_INTERVAL=10         # Сохранять каждые N страниц
PAGE_DELAY=2.5                 # Задержка между страницами (сек)
MAX_RETRIES=3                  # Количество повторных попыток
RETRY_DELAY=5                  # Задержка между попытками (сек)
```

### Настройки браузера

```env
HEADLESS_MODE=true             # Headless режим (true/false)
BROWSER_TYPE=chromium          # Тип браузера (chromium/firefox/webkit)
PAGE_TIMEOUT=60000             # Таймаут страницы (мс)
```

### Пути

```env
OUTPUT_DIR=output              # Директория для JSON файлов
LOG_DIR=logs                   # Директория для логов
OUTPUT_FILENAME_PREFIX=goszakup_links  # Префикс имени файла
```

### Логирование

```env
LOG_LEVEL=INFO                 # Уровень логирования (DEBUG/INFO/WARNING/ERROR)
```

## Примеры использования

### Пример 1: Быстрый тест (10 ссылок)

```bash
python scraper.py --max-links 10 --verbose
```

### Пример 2: Парсинг 2500 ссылок

```bash
python scraper.py --max-links 2500
```

Результат:
- Обработает 2 страницы (2000 + 500 ссылок)
- Сохранит контрольную точку после первой страницы
- Создаст файл `output/goszakup_links_*.json`

### Пример 3: Парсинг с видимым браузером (отладка)

```bash
python scraper.py --max-links 100 --no-headless --verbose
```

Полезно для:
- Отладки работы скрипта
- Визуального контроля процесса
- Проверки правильности работы селекторов

### Пример 4: Парсинг без лимита (до конца)

```bash
python scraper.py --max-links 1000000
```

Будет парсить до тех пор, пока не закончатся страницы.

## Структура проекта

```
c:\CODES\Tender2_additional\goszakup_tru_code_parsing\
├── scraper.py              # Основной скрипт парсера
├── .env                    # Конфигурация (не в git)
├── .env.example            # Шаблон конфигурации
├── requirements.txt        # Зависимости Python
├── README.md               # Документация
├── 1.py                    # Простой scraper (для справки)
├── page.html               # Пример HTML страницы (для тестов)
├── output/                 # Директория с результатами JSON
│   └── goszakup_links_*.json
└── logs/                   # Директория с логами
    └── scraper_*.log
```

## Решение проблем

### Проблема: "playwright not found"

```bash
pip install playwright
playwright install chromium
```

### Проблема: Timeout при загрузке страницы

Увеличьте `PAGE_TIMEOUT` в `.env`:

```env
PAGE_TIMEOUT=120000  # 2 минуты
```

### Проблема: Не находит dropdown элемент

1. Проверьте скриншот в `logs/error_dropdown_*.png`
2. Запустите с `--no-headless --verbose` для визуальной отладки
3. Убедитесь, что страница загружается корректно

### Проблема: Rate limiting / блокировка

Увеличьте задержку между страницами в `.env`:

```env
PAGE_DELAY=5.0  # 5 секунд между страницами
```

### Проблема: Не все ссылки парсятся

1. Проверьте логи в `logs/scraper_*.log`
2. Убедитесь, что селектор корректен: `a[target="_blank"][style="font-size: 13px"]`
3. Проверьте, что страница полностью загружена перед парсингом

## Технические детали

### Алгоритм работы

1. Запуск браузера Playwright
2. Переход на первую страницу
3. Изменение dropdown на 2000 записей (только на первой странице)
4. Ожидание перезагрузки страницы (AJAX)
5. Извлечение всех ссылок с помощью CSS селектора
6. Конвертация относительных URL в абсолютные
7. Удаление дубликатов (через set)
8. Сохранение контрольной точки (каждые N страниц)
9. Задержка перед следующей страницей
10. Переход к пункту 4 для следующей страницы
11. Остановка при достижении лимита или конца результатов
12. Финальное сохранение в JSON

### CSS Селектор

Скрипт использует CSS селектор для поиска ссылок:

```css
a[target="_blank"][style="font-size: 13px"]
```

Это находит все элементы `<a>` с атрибутами:
- `target="_blank"`
- `style="font-size: 13px"`

### Удаление дубликатов

Дубликаты автоматически удаляются с помощью Python `set()`:
- Одна и та же ссылка может встречаться несколько раз на одной странице
- Одна и та же ссылка может появляться на разных страницах
- Set гарантирует уникальность всех собранных ссылок

### Атомарная запись

Для предотвращения повреждения данных при сбоях используется атомарная запись:
1. Запись во временный файл `.tmp`
2. Переименование в финальное имя

Это гарантирует, что файл всегда будет корректным.

## Лицензия

Проект создан для образовательных целей.

## Поддержка

При возникновении проблем:
1. Проверьте логи в `logs/scraper_*.log`
2. Запустите с `--verbose` для подробной информации
3. Проверьте скриншоты ошибок в `logs/error_*.png`
